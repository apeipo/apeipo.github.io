<!DOCTYPE html><html><head><meta charset="utf-8"><title>逻辑回归思考 | linxianlong的博客</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="keywords" content="MachineLearning,"><meta name="description" content="参考链接：logistic回归详解一：为什么要使用logistic函数logistic回归详解二：损失函数（cost function）详解*浅析Logistic Regression从线性回归到逻辑回归对于线性回归来说，模型的假设是输入向量x和真实值y之间存在关系：y = wx + b，即用wx + b去逼近样本的真实值。拓展到广义的线性模型（机器学习第三章P56），可以令wx + b去逼近y的"><meta name="keywords" content="MachineLearning"><meta property="og:type" content="article"><meta property="og:title" content="逻辑回归思考"><meta property="og:url" content="http://longlog.me/2017/06/14/ml-logic/index.html"><meta property="og:site_name" content="linxianlong的博客"><meta property="og:description" content="参考链接：logistic回归详解一：为什么要使用logistic函数logistic回归详解二：损失函数（cost function）详解*浅析Logistic Regression从线性回归到逻辑回归对于线性回归来说，模型的假设是输入向量x和真实值y之间存在关系：y = wx + b，即用wx + b去逼近样本的真实值。拓展到广义的线性模型（机器学习第三章P56），可以令wx + b去逼近y的"><meta property="og:locale" content="default"><meta property="og:image" content="http://7xrhmq.com1.z0.glb.clouddn.com/2017-06-14-14973232448037.jpg"><meta property="og:image" content="http://7xrhmq.com1.z0.glb.clouddn.com/2017-06-14-14972714947069.jpg"><meta property="og:image" content="http://7xrhmq.com1.z0.glb.clouddn.com/2017-06-14-14973239587272.jpg"><meta property="og:image" content="http://7xrhmq.com1.z0.glb.clouddn.com/2017-06-14-14973241686186.jpg"><meta property="og:image" content="http://7xrhmq.com1.z0.glb.clouddn.com/2017-06-14-14973241924422.jpg"><meta property="og:updated_time" content="2018-08-06T12:12:41.824Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="逻辑回归思考"><meta name="twitter:description" content="参考链接：logistic回归详解一：为什么要使用logistic函数logistic回归详解二：损失函数（cost function）详解*浅析Logistic Regression从线性回归到逻辑回归对于线性回归来说，模型的假设是输入向量x和真实值y之间存在关系：y = wx + b，即用wx + b去逼近样本的真实值。拓展到广义的线性模型（机器学习第三章P56），可以令wx + b去逼近y的"><meta name="twitter:image" content="http://7xrhmq.com1.z0.glb.clouddn.com/2017-06-14-14973232448037.jpg"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" href="/css/styles.css"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="//hm.baidu.com/hm.js?bec693dc99391bbe29dd7310e3fff81a";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script></head></html><body><div class="post-header LEFT"><div class="toolbox"><ul class="list-toolbox"><li class="item-toolbox"><a class="CIRCLE" href="/">首页</a></li><li class="item-toolbox"><a class="CIRCLE" href="/categories/">分类</a></li><li class="item-toolbox"><a class="CIRCLE" href="/tags/">标签</a></li><li class="item-toolbox"><a class="CIRCLE" href="/2017/03/28/MY-2017/">2017</a></li><li class="item-toolbox"><a class="CIRCLE" href="/2018/08/06/2018-Target/">2018</a></li><li class="item-toolbox"><a class="CIRCLE" href="/link/">友链</a></li><li class="item-toolbox"><a class="CIRCLE" href="/about/">关于</a></li></ul></div><div class="toolbox tool-vertical" id="tool-vertical"><ul class="list-toolbox list-vertical"><li class="item-toolbox"><a class="CIRCLE VERTICAL" href="/">首页</a></li><li class="item-toolbox"><a class="CIRCLE VERTICAL" href="/categories/">分类</a></li><li class="item-toolbox"><a class="CIRCLE VERTICAL" href="/tags/">标签</a></li><li class="item-toolbox"><a class="CIRCLE VERTICAL" href="/2017/03/28/MY-2017/">2017</a></li><li class="item-toolbox"><a class="CIRCLE VERTICAL" href="/2018/08/06/2018-Target/">2018</a></li><li class="item-toolbox"><a class="CIRCLE VERTICAL" href="/link/">友链</a></li><li class="item-toolbox"><a class="CIRCLE VERTICAL" href="/about/">关于</a></li></ul></div><script type="text/javascript">function getStyle(e){return void 0===e.currentStyle?getComputedStyle(e):e.currentStyle}window.onscroll=function(){var e=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop,t=document.getElementsByClassName("toolbox")[0],l=document.getElementById("tool-vertical"),o=getStyle(l).display,n=document.getElementsByClassName("CIRCLE VERTICAL");if(60<=e&&"none"==o){l.style.display="block",t.style.display="none";for(var s=0;s<n.length;s++)n[s].setAttribute("class","CIRCLE VERTICAL")}if(e<60&&"block"==o)for(t.style.display="block",setTimeout("document.getElementById('tool-vertical').style.display = 'none'",600),s=0;s<n.length;s++)n[s].setAttribute("class",n[s].className+" a-vertical")}</script></div><div id="toc" class="toc-article"><strong class="toc-title">文章目录</strong><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#从线性回归到逻辑回归"><span class="toc-text">从线性回归到逻辑回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#为什么要用sigmoid函数？"><span class="toc-text">为什么要用sigmoid函数？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#逻辑回归损失函数怎么来的？"><span class="toc-text">逻辑回归损失函数怎么来的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#最小二乘、极大似然、梯度下降"><span class="toc-text">最小二乘、极大似然、梯度下降</span></a></li></ol></div><div class="content-post LEFT"><article id="post-ml-logic" class="article article-type-post" itemscope itemprop="blogPost"><header class="article-header"><h1 class="post-title">逻辑回归思考</h1><div class="article-meta"><span>2017-06-14</span> <span>| </span><span class="article-author">Linxianlong</span> <span>| </span><span class="article-category"><a class="article-category-link" href="/categories/机器学习/">机器学习</a></span></div></header><div class="article-content"><p>参考链接：<br><a href="http://blog.csdn.net/bitcarmanlee/article/details/51154481" target="_blank" rel="noopener">logistic回归详解一：为什么要使用logistic函数</a><br><a href="http://blog.csdn.net/bitcarmanlee/article/details/51165444" target="_blank" rel="noopener">logistic回归详解二：损失函数（cost function）详解</a><br><a href="https://chenrudan.github.io/blog/2016/01/09/logisticregression.html" target="_blank" rel="noopener">*浅析Logistic Regression</a></p><h2 id="从线性回归到逻辑回归"><a href="#从线性回归到逻辑回归" class="headerlink" title="从线性回归到逻辑回归"></a>从线性回归到逻辑回归</h2><p>对于线性回归来说，模型的假设是输入向量x和真实值y之间存在关系：y = wx + b，即用wx + b去逼近样本的真实值。<br>拓展到广义的线性模型（机器学习第三章P56），可以令wx + b去逼近y的衍生函数，例如 ln^y = wx + b。<br><img src="http://7xrhmq.com1.z0.glb.clouddn.com/2017-06-14-14973232448037.jpg" alt=""><br>对于二分类问题，逻辑回归提出的假设是，令wx + b逼近样本为正例的概率。参考上图，即假设：P = g^-1 * (wx + b)。此时，需要一个合适的函数g，能将wx + b 映射到概率[0, 1]之间，因此引入了sigmoid函数。</p><h2 id="为什么要用sigmoid函数？"><a href="#为什么要用sigmoid函数？" class="headerlink" title="为什么要用sigmoid函数？"></a>为什么要用sigmoid函数？</h2><p>理想的模型函数是如图3.2的红色函数，其中z = wx + b，即线性模型的预测结果。如果z&gt;0则判定为正例，z&lt;0则为反例。<br><img src="http://7xrhmq.com1.z0.glb.clouddn.com/2017-06-14-14972714947069.jpg" alt="-w500"><br>但是，这个函数存在不连续，在z=0的点不可导等特点，数学上处理起来不方便。而<strong>sigmoid函数，刚好满足二分类的需求，并且具有良好的连续性</strong>。</p><blockquote><p>线性函数的值越接近正无穷，概率值就越接近1；线性值越接近负无穷，概率值越接近0，这样的模型是逻辑斯蒂回归模型(李航.《统计机器学习》)</p></blockquote><h2 id="逻辑回归损失函数怎么来的？"><a href="#逻辑回归损失函数怎么来的？" class="headerlink" title="逻辑回归损失函数怎么来的？"></a>逻辑回归损失函数怎么来的？</h2><p>机器学习或者统计机器学习常见的损失函数：</p><ol><li>平方损失函数（quadratic loss function): L(Y,f(X))=(Y−f(x))^2</li><li>绝对值损失函数(absolute loss function): L(Y,f(x))=|Y−f(X)|</li><li>对数损失函数（logarithmic loss function) 或<strong>对数似然损失函数</strong>(log-likehood loss function):<br>L(Y,P(Y|X))=−logP(Y|X), P(Y|X)表示样本X标记正确的概率。<br>逻辑回归使用的是对数损失函数，根据对数损失定义，可以得出单个样本的损失函数如下：</li></ol><p><img src="http://7xrhmq.com1.z0.glb.clouddn.com/2017-06-14-14973239587272.jpg" alt="-w400"><br>将两个式子合并为一个：</p><p><img src="http://7xrhmq.com1.z0.glb.clouddn.com/2017-06-14-14973241686186.jpg" alt="-w400"></p><p>全体样本的损失函数可以表示为：</p><p><img src="http://7xrhmq.com1.z0.glb.clouddn.com/2017-06-14-14973241924422.jpg" alt="-w400"></p><p>定义损失函数之后，剩下的就是求解的过程，逻辑回归求解方法有梯度下降、牛顿法、BFGS等。</p><h2 id="最小二乘、极大似然、梯度下降"><a href="#最小二乘、极大似然、梯度下降" class="headerlink" title="最小二乘、极大似然、梯度下降"></a>最小二乘、极大似然、梯度下降</h2><p><a href="https://www.zhihu.com/question/24900876" target="_blank" rel="noopener">知乎问题</a><br>机器学习的框架是：<strong>模型、目标和算法</strong>。模型是指输入到输出的映射方式，目标是让模型”更好”地拟合数据，或者说让模型损失(Cost Function)达到最小，算法是指达到目标使用的方法。<br>最小二乘和梯度下降都属于求解方法（优化算法），最小二乘一般用于线性模型，梯度下降很多模型都会使用，比如逻辑回归（线性模型也可以用梯度下降）。<br>极大似然法是一种推导方式，使用极大似然法可以从概率的角度推导出模型的损失函数（比如逻辑回归的对数似然）。</p><blockquote><p>极大似然估计：在已经得到试验结果的情况下，我们应该寻找<strong>使这个结果出现的可能性最大的那个theta作为真实theta的估计</strong></p></blockquote><p>对应到逻辑回归中，就是寻找参数theta，使得每个样本属于其真实标记的概率越大越好（机器学习 - P59）</p></div></article><section class="disqus-comments"><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div></section></div><script type="text/javascript">!function(){var t=document.getElementsByTagName("article")[0];if(null!=t){imgs=t.getElementsByTagName("img");for(var e=0;e<imgs.length;e++)img=imgs[e],width=parseInt(img.getAttribute("alt").replace("-w","")),0<width&&img.setAttribute("style","width:"+width+"px")}}()</script><script>var disqus_shortname="apeipo",disqus_url="http://longlog.me/2017/06/14/ml-logic/";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}()</script></body>