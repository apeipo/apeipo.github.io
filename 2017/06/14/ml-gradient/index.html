<!DOCTYPE html><html><head><meta charset="utf-8"><title>梯度下降笔记 | linxianlong的博客</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="keywords" content="MachineLearning,"><meta name="description" content="关于梯度下降两篇比较易懂的文章：从导数谈起线性回归及梯度下降导数一个函数在某一点的导数描述了这个函数在这一点附近的变化率先从1维上来说，例如对于函数y = x^2 。 要求得他的极值点，很简单：对x求导 y’ = 2x，令2x = 0，求解得到当x=0时，y取得极值。如果拓展到2维呢？如 z(x, y) = x^2 + y^2 。还是一样的方法，不同的是需要对x和y分别求偏导。z’(x) = 2x"><meta name="keywords" content="MachineLearning"><meta property="og:type" content="article"><meta property="og:title" content="梯度下降笔记"><meta property="og:url" content="http://longlog.me/2017/06/14/ml-gradient/index.html"><meta property="og:site_name" content="linxianlong的博客"><meta property="og:description" content="关于梯度下降两篇比较易懂的文章：从导数谈起线性回归及梯度下降导数一个函数在某一点的导数描述了这个函数在这一点附近的变化率先从1维上来说，例如对于函数y = x^2 。 要求得他的极值点，很简单：对x求导 y’ = 2x，令2x = 0，求解得到当x=0时，y取得极值。如果拓展到2维呢？如 z(x, y) = x^2 + y^2 。还是一样的方法，不同的是需要对x和y分别求偏导。z’(x) = 2x"><meta property="og:locale" content="default"><meta property="og:image" content="https://longlog-1300108443.cos.ap-beijing.myqcloud.com/before2019/2017-06-14-14974242248427.jpg"><meta property="og:image" content="https://longlog-1300108443.cos.ap-beijing.myqcloud.com/before2019/2017-06-14-14974243096274.jpg"><meta property="og:image" content="https://longlog-1300108443.cos.ap-beijing.myqcloud.com/before2019/2017-06-14-14974245759806.jpg"><meta property="og:image" content="https://longlog-1300108443.cos.ap-beijing.myqcloud.com/before2019/2017-06-14-14974267407348.jpg"><meta property="og:image" content="https://longlog-1300108443.cos.ap-beijing.myqcloud.com/before2019/2017-06-14-14974280777979.jpg"><meta property="og:image" content="https://longlog-1300108443.cos.ap-beijing.myqcloud.com/before2019/2017-06-14-14974284080883.jpg"><meta property="og:image" content="https://longlog-1300108443.cos.ap-beijing.myqcloud.com/before2019/2017-06-14-14974298909255.png"><meta property="og:updated_time" content="2019-08-29T07:09:16.068Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="梯度下降笔记"><meta name="twitter:description" content="关于梯度下降两篇比较易懂的文章：从导数谈起线性回归及梯度下降导数一个函数在某一点的导数描述了这个函数在这一点附近的变化率先从1维上来说，例如对于函数y = x^2 。 要求得他的极值点，很简单：对x求导 y’ = 2x，令2x = 0，求解得到当x=0时，y取得极值。如果拓展到2维呢？如 z(x, y) = x^2 + y^2 。还是一样的方法，不同的是需要对x和y分别求偏导。z’(x) = 2x"><meta name="twitter:image" content="https://longlog-1300108443.cos.ap-beijing.myqcloud.com/before2019/2017-06-14-14974242248427.jpg"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" href="/css/styles.css"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="//hm.baidu.com/hm.js?bec693dc99391bbe29dd7310e3fff81a";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script></head></html><body><div class="post-header LEFT"><div class="toolbox"><ul class="list-toolbox"><li class="item-toolbox"><a class="CIRCLE" href="/">首页</a></li><li class="item-toolbox"><a class="CIRCLE" href="/categories/">分类</a></li><li class="item-toolbox"><a class="CIRCLE" href="/tags/">标签</a></li><li class="item-toolbox"><a class="CIRCLE" href="/link/">友链</a></li><li class="item-toolbox"><a class="CIRCLE" href="/about/">关于</a></li></ul></div><div class="toolbox tool-vertical" id="tool-vertical"><ul class="list-toolbox list-vertical"><li class="item-toolbox"><a class="CIRCLE VERTICAL" href="/">首页</a></li><li class="item-toolbox"><a class="CIRCLE VERTICAL" href="/categories/">分类</a></li><li class="item-toolbox"><a class="CIRCLE VERTICAL" href="/tags/">标签</a></li><li class="item-toolbox"><a class="CIRCLE VERTICAL" href="/link/">友链</a></li><li class="item-toolbox"><a class="CIRCLE VERTICAL" href="/about/">关于</a></li></ul></div><script type="text/javascript">function getStyle(e){return void 0===e.currentStyle?getComputedStyle(e):e.currentStyle}window.onscroll=function(){var e=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop,t=document.getElementsByClassName("toolbox")[0],l=document.getElementById("tool-vertical"),o=getStyle(l).display,n=document.getElementsByClassName("CIRCLE VERTICAL");if(60<=e&&"none"==o){l.style.display="block",t.style.display="none";for(var s=0;s<n.length;s++)n[s].setAttribute("class","CIRCLE VERTICAL")}if(e<60&&"block"==o)for(t.style.display="block",setTimeout("document.getElementById('tool-vertical').style.display = 'none'",600),s=0;s<n.length;s++)n[s].setAttribute("class",n[s].className+" a-vertical")}</script></div><div id="toc" class="toc-article"><strong class="toc-title">文章目录</strong><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#导数"><span class="toc-text">导数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#梯度"><span class="toc-text">梯度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#梯度下降"><span class="toc-text">梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#梯度下降步骤"><span class="toc-text">梯度下降步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#注意点"><span class="toc-text">注意点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#梯度下降的种类"><span class="toc-text">梯度下降的种类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#BGD（批量梯度下降）"><span class="toc-text">BGD（批量梯度下降）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SGD（随机梯度下降）"><span class="toc-text">SGD（随机梯度下降）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#SGD和BGD的伪代码"><span class="toc-text">SGD和BGD的伪代码</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MBGD（小批量梯度下降法）"><span class="toc-text">MBGD（小批量梯度下降法）</span></a></li></ol></li></ol></div><div class="content-post LEFT"><article id="post-ml-gradient" class="article article-type-post" itemscope itemprop="blogPost"><header class="article-header"><h1 class="post-title">梯度下降笔记</h1><div class="article-meta"><span>2017-06-14</span> <span>| </span><span class="article-author">Linxianlong</span> <span>| </span><span class="article-category"><a class="article-category-link" href="/categories/机器学习/">机器学习</a></span></div></header><div class="article-content"><p>关于梯度下降两篇比较易懂的文章：<br><a href="http://www.cnblogs.com/jianxinzhou/p/3950518.html" target="_blank" rel="noopener">从导数谈起</a><br><a href="http://blog.csdn.net/xiazdong/article/details/7950084" target="_blank" rel="noopener">线性回归及梯度下降</a></p><h2 id="导数"><a href="#导数" class="headerlink" title="导数"></a>导数</h2><blockquote><p>一个函数在某一点的导数描述了这个函数在这一点附近的变化率</p></blockquote><p>先从1维上来说，例如对于函数y = x^2 。 要求得他的极值点，很简单：对x求导 y’ = 2x，令2x = 0，求解得到当x=0时，y取得极值。<br>如果拓展到2维呢？如 z(x, y) = x^2 + y^2 。还是一样的方法，不同的是需要对x和y分别求<strong>偏导</strong>。<br>z’(x) = 2x ; z’(y) = 2y。令z’(x)和z’(y)等于0，可以解方程得到z(x, y)的极值点为（0, 0）。<br>同样的方法可以拓展到多维，对每个变量求偏导，并分别求得偏导为0时的变量值，结果向量就是函数的极值点。</p><h2 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h2><p>导数描述了函数的变化率<br>对于平面上的曲线，一个点的导数可以理解该点切线的斜率，切线的正方向，是函数的上升方向。<br>对于多维的空间，就是该点的梯度，梯度的正方向，是函数的上升反向，反之就是下降方向。</p><blockquote><p>对于函数F(X)（X = {x0,x1,…,xn})而言，梯度是F(X)对各个分量求偏导后的结果,代表了F(X)在各个方向的变化率</p></blockquote><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>既然让导数为0可以得到函数的极值点，那为什么要梯度下降？<br>因为很多时候，我们知道导数的表达式，但是没法求导数为0的方程（比如导数是 exp(x)+(ln(x))^2 + x^5 这种..）。这种情况下，我们只能根据根据每个点的导数值（即梯度），选择梯度的反方向去逼近函数的极小值点，这种方法就是梯度下降，反之就是梯度上升。</p><h3 id="梯度下降步骤"><a href="#梯度下降步骤" class="headerlink" title="梯度下降步骤"></a>梯度下降步骤</h3><p>例如对于单变量线性回归（theta0 + theta1 * x）的损失函数：<br><img src="https://longlog-1300108443.cos.ap-beijing.myqcloud.com/before2019/2017-06-14-14974242248427.jpg" alt="-w300"><br>其梯度下降的方法：<br>(1)先确定向下一步的步伐大小，称为Learning rate；<br>(2)任意给定一个初始值：theta0, theta1；<br>(3)确定一个向下的方向，并向下走预先规定的步伐，并更新；<br>(4)当下降的高度小于某个定义的值，则停止下降；<br><img src="https://longlog-1300108443.cos.ap-beijing.myqcloud.com/before2019/2017-06-14-14974243096274.jpg" alt="-w392"></p><h3 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h3><ol><li>初始点不同，获得的最小值也不同，因此梯度下降求得的只是局部最小值</li><li>降的步伐大小非常重要，因为如果太小，则找到函数最小值的速度就很慢，如果太大，有可能呈之字型下降。如图：<br><img src="https://longlog-1300108443.cos.ap-beijing.myqcloud.com/before2019/2017-06-14-14974245759806.jpg" alt="-w200"></li><li>如果Learning rate取值后发现J function 增长了，则需要减小Learning rate的值。</li><li>梯度下降是通过不停的迭代，为了减少迭代次数，因此引入了Feature Scaling（<strong>特征标准化</strong>），使得取值范围<strong>大致</strong>都在-1&lt;=x&lt;=1之间。</li></ol><p>如果不同scale的特征，没有进行标准化，例如某个特征范围是[0, 1]，而另外一个特征是[100w, 200w]。可以想象，等高线的图会特别扁或者特别细。此时，梯度下降的步长得取得特别小，会导致另外一个特征的收敛特别慢。</p><h2 id="梯度下降的种类"><a href="#梯度下降的种类" class="headerlink" title="梯度下降的种类"></a>梯度下降的种类</h2><p><a href="http://www.cnblogs.com/maybe2030/p/5089753.html" target="_blank" rel="noopener">梯度下降法的三种形式BGD、SGD以及MBGD</a><br>这篇文章有详细的介绍，这里只进行简单的总结。</p><h3 id="BGD（批量梯度下降）"><a href="#BGD（批量梯度下降）" class="headerlink" title="BGD（批量梯度下降）"></a>BGD（批量梯度下降）</h3><p>最原始的梯度下降方法， 每次迭代时都使用所有的样本来进行更新。<br>对于上文中线性回归的损失函数，对每个变量求偏导后可以得出梯度下降的变化式：<br><img src="https://longlog-1300108443.cos.ap-beijing.myqcloud.com/before2019/2017-06-14-14974267407348.jpg" alt="-w300"><br>可以看出，每次参数的迭代，都需要全体的样本参与。由于是全体参与迭代，参数稳定收敛，但是在训练集很大时，训练效率会非常低。</p><h3 id="SGD（随机梯度下降）"><a href="#SGD（随机梯度下降）" class="headerlink" title="SGD（随机梯度下降）"></a>SGD（随机梯度下降）</h3><p><a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" target="_blank" rel="noopener">维基百科</a><br>利用单个样本来进行迭代，解决样本量大时BGD迭代速度慢的问题。<br>如上文，将线性回归的损失函数进行转换：<br><img src="https://longlog-1300108443.cos.ap-beijing.myqcloud.com/before2019/2017-06-14-14974280777979.jpg" alt="-w400"><br>这个式子可以解释为，整体的损失可以拆解为单个样本的损失之和，如果能将单个样本的损失减小，则总的损失也会减小（<strong>减小单个样本的损失来逼近总体损失的极小值</strong>）。<br><strong>可以将SGD理解样本全集只有1个时的BGD。</strong><br>对单个样本的cost函数求偏导后得出theta的变化式：<br><img src="https://longlog-1300108443.cos.ap-beijing.myqcloud.com/before2019/2017-06-14-14974284080883.jpg" alt="-w300"><br>SGD每次迭代只有一个样本参与，因此可以大大增加迭代速度，提升训练速度。但是由于每次迭代都是局部进行，迭代比较”盲目”。学习过程，目标函数可能会存在波动：<br><img src="https://longlog-1300108443.cos.ap-beijing.myqcloud.com/before2019/2017-06-14-14974298909255.png" alt="-w200"><br>SGD 收敛过程中的波动，可能会帮助目标函数跳入另一个可能的更小的极小值，但是也有可能跳出原来全局的极小值。</p><h4 id="SGD和BGD的伪代码"><a href="#SGD和BGD的伪代码" class="headerlink" title="SGD和BGD的伪代码"></a>SGD和BGD的伪代码</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#BGD</span></span><br><span class="line">choose initial vector of parameters <span class="keyword">and</span> learning rate</span><br><span class="line">Repeat until an approximate minimum <span class="keyword">is</span> obtained:</span><br><span class="line">  params_grad = evaluate_gradient(loss_function, data, params)</span><br><span class="line">  params = params - learning_rate * params_grad</span><br><span class="line"></span><br><span class="line"><span class="comment">#SGD</span></span><br><span class="line">choose initial vector of parameters <span class="keyword">and</span> learning rate</span><br><span class="line">Repeat until an approximate minimum <span class="keyword">is</span> obtained:</span><br><span class="line">  np.random.shuffle(data)</span><br><span class="line">  <span class="keyword">for</span> example <span class="keyword">in</span> data:</span><br><span class="line">    params_grad = evaluate_gradient(loss_function, example, params)</span><br><span class="line">    params = params - learning_rate * params_grad</span><br></pre></td></tr></table></figure><h3 id="MBGD（小批量梯度下降法）"><a href="#MBGD（小批量梯度下降法）" class="headerlink" title="MBGD（小批量梯度下降法）"></a>MBGD（小批量梯度下降法）</h3><p>SGD和BGD的折衷方法，即每次迭代使用指定数量的一批样本。</p></div></article><section class="disqus-comments"><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div></section></div><script type="text/javascript">!function(){var t=document.getElementsByTagName("article")[0];if(null!=t){imgs=t.getElementsByTagName("img");for(var e=0;e<imgs.length;e++)img=imgs[e],width=parseInt(img.getAttribute("alt").replace("-w","")),0<width&&img.setAttribute("style","width:"+width+"px")}}()</script><script>var disqus_shortname="apeipo",disqus_url="http://longlog.me/2017/06/14/ml-gradient/";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}()</script></body>